{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3)\n",
      "(10000, 32, 32, 3)\n",
      "(50000, 10)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow.keras import layers, optimizers, datasets, Sequential, losses\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "def load_dataset():\n",
    "    # 在线下载，加载 CIFAR10 数据集\n",
    "    (X_train, y_train), (X_test, y_test) = datasets.cifar10.load_data()\n",
    "    # 删除 y 的一个维度， [b,1] => [b]\n",
    "    y_train = tf.squeeze(y_train, axis=1)\n",
    "    y_test = tf.squeeze(y_test, axis=1)\n",
    "    # 构建训练集对象，随机打乱，预处理，批量化\n",
    "    \"\"\"\n",
    "    tf.data.Dataset.from_tensor_slices()的输入可以是numpy也可以是tensor，如果是numpy会自动转化为tensor。\n",
    "    tf.data.Dataset.shuffle()函数的作用是打乱数据，参数为缓冲区的数据条数\n",
    "    map表示预处理,参数为与处理函数\n",
    "    \"\"\"\n",
    "    print(X_test.shape)\n",
    "    train_db = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "    train_db = train_db.shuffle(1000).map(preprocess).batch(128)\n",
    "    # 构建测试集对象，预处理，批量化\n",
    "    test_db = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "    test_db = test_db.map(preprocess).batch(64)\n",
    "    # 从训练集中采样一个 Batch，并观察\n",
    "    sample = next(iter(train_db))\n",
    "    #print('sample:', sample[0].shape, sample[1].shape, tf.reduce_min(sample[0]), tf.reduce_max(sample[0]))\n",
    "    return train_db, test_db\n",
    "\n",
    "def preprocess(x, y):\n",
    "    x = 2 * tf.cast(x, dtype=tf.float32) / 255. - 1\n",
    "    y = tf.cast(y, dtype=tf.int32)\n",
    "    return x, y\n",
    "\n",
    "def load_dataset1():\n",
    "    # 在线下载，加载 CIFAR10 数据集\n",
    "    (X_train, y_train), (X_test, y_test) = datasets.cifar10.load_data()\n",
    "    # 删除 y 的一个维度， [b,1] => [b]\n",
    "    #y_train = tf.squeeze(y_train, axis=1)\n",
    "    #y_test = tf.squeeze(y_test, axis=1)\n",
    "    #(X_train, y_train) = shuffle((X_train, y_train))\n",
    "    X_train, y_train = preprocess(X_train, y_train)\n",
    "    X_test, y_test = preprocess(X_test, y_test)\n",
    "    y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
    "    y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "#train_db, test_db = load_dataset()\n",
    "X_train, y_train, X_test, y_test = load_dataset1()\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def network_model():\n",
    "    model = Sequential()\n",
    "    # 原始图片结构32x32x3\n",
    "    model.add(layers.Conv2D(64, kernel_size=(3, 3), strides=(1, 1), padding='SAME', activation=\"relu\", input_shape=[32, 32, 3]))\n",
    "    model.add(layers.Conv2D(64, kernel_size=(3, 3), strides=(1, 1), padding='SAME', activation=\"relu\"))\n",
    "\n",
    "    model.add(layers.MaxPooling2D(pool_size=[2, 2], strides=2, padding=\"SAME\"))\n",
    "\n",
    "    # 图片结构16x16x64\n",
    "    model.add(layers.Conv2D(128, kernel_size=(3, 3), strides=(1, 1), padding='SAME', activation=\"relu\"))\n",
    "    model.add(layers.Conv2D(128, kernel_size=(3, 3), strides=(1, 1), padding='SAME', activation=\"relu\"))\n",
    "\n",
    "    model.add(layers.MaxPooling2D(pool_size=[2, 2], strides=2, padding=\"SAME\"))\n",
    "\n",
    "    # 图片结构 8x8x128\n",
    "    model.add(layers.Conv2D(256, kernel_size=(3, 3), strides=(1, 1), padding='SAME', activation=\"relu\"))\n",
    "    model.add(layers.Conv2D(256, kernel_size=(3, 3), strides=(1, 1), padding='SAME', activation=\"relu\"))\n",
    "    model.add(layers.Conv2D(256, kernel_size=(3, 3), strides=(1, 1), padding='SAME', activation=\"relu\"))\n",
    "\n",
    "    model.add(layers.MaxPooling2D(pool_size=[2, 2], strides=2, padding=\"SAME\"))\n",
    "\n",
    "    # 图片结构4x4x256\n",
    "    model.add(layers.Conv2D(512, kernel_size=(3, 3), strides=(1, 1), padding='SAME', activation=\"relu\"))\n",
    "    model.add(layers.Conv2D(512, kernel_size=(3, 3), strides=(1, 1), padding='SAME', activation=\"relu\"))\n",
    "    model.add(layers.Conv2D(512, kernel_size=(3, 3), strides=(1, 1), padding='SAME', activation=\"relu\"))\n",
    "\n",
    "    # 少进行一次池化\n",
    "    # model.add(layers.MaxPooling2D(pool_size=[2, 2], strides=2, padding=\"SAME\"))\n",
    "\n",
    "    # 图片结构4x4x512\n",
    "    model.add(layers.Conv2D(512, kernel_size=(3, 3), strides=(1, 1), padding='SAME', activation=\"relu\"))\n",
    "    model.add(layers.Conv2D(512, kernel_size=(3, 3), strides=(1, 1), padding='SAME', activation=\"relu\"))\n",
    "    model.add(layers.Conv2D(512, kernel_size=(3, 3), strides=(1, 1), padding='SAME', activation=\"relu\"))\n",
    "\n",
    "    model.add(layers.MaxPooling2D(pool_size=[2, 2], strides=2, padding=\"SAME\"))\n",
    "\n",
    "    # 图片结构2x2x512\n",
    "    model.add(layers.Flatten())     # 把上层输入拉平\n",
    "    model.add(layers.Dense(units=256, activation=\"relu\"))\n",
    "    model.add(layers.Dense(units=128, activation=\"relu\"))\n",
    "    model.add(layers.Dense(units=10, activation=\"softmax\"))\n",
    "\n",
    "   # model.summary()\n",
    "\n",
    "    return model\n",
    "\n",
    "def VGG16():\n",
    "    weight_decay = 0.001\n",
    "    num_classes = 10\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(layers.Conv2D(64, (3, 3), padding='same',\n",
    "                         input_shape=[32, 32, 3], kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(layers.Activation('relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Conv2D(64, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(layers.Activation('relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(layers.Conv2D(128, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(layers.Activation('relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dropout(0.4))\n",
    "\n",
    "    model.add(layers.Conv2D(128, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(layers.Activation('relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(layers.Conv2D(256, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(layers.Activation('relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dropout(0.4))\n",
    "\n",
    "    model.add(layers.Conv2D(256, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(layers.Activation('relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dropout(0.4))\n",
    "\n",
    "    model.add(layers.Conv2D(256, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(layers.Activation('relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "\n",
    "    model.add(layers.Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(layers.Activation('relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dropout(0.4))\n",
    "\n",
    "    model.add(layers.Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(layers.Activation('relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dropout(0.4))\n",
    "\n",
    "    model.add(layers.Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(layers.Activation('relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "\n",
    "    model.add(layers.Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(layers.Activation('relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dropout(0.4))\n",
    "\n",
    "    model.add(layers.Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(layers.Activation('relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dropout(0.4))\n",
    "\n",
    "    model.add(layers.Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(layers.Activation('relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(512,kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(layers.Activation('relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(num_classes))\n",
    "    model.add(layers.Activation('softmax'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VGG16()\n",
    "model.compile(loss=tf.keras.losses.categorical_crossentropy, optimizer=optimizers.Adam(lr=0.01), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "41472/50000 [=======================>......] - ETA: 6s - loss: 16.4454 - accuracy: 0.1793"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train, batch_size=128, epochs=10, validation_data=(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
