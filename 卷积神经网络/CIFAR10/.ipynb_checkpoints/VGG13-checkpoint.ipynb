{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3)\n",
      "(10000, 32, 32, 3)\n",
      "(50000, 10)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow.keras import layers, optimizers, datasets, Sequential, losses\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "def load_dataset():\n",
    "    # 在线下载，加载 CIFAR10 数据集\n",
    "    (X_train, y_train), (X_test, y_test) = datasets.cifar10.load_data()\n",
    "    # 删除 y 的一个维度， [b,1] => [b]\n",
    "    y_train = tf.squeeze(y_train, axis=1)\n",
    "    y_test = tf.squeeze(y_test, axis=1)\n",
    "    # 构建训练集对象，随机打乱，预处理，批量化\n",
    "    \"\"\"\n",
    "    tf.data.Dataset.from_tensor_slices()的输入可以是numpy也可以是tensor，如果是numpy会自动转化为tensor。\n",
    "    tf.data.Dataset.shuffle()函数的作用是打乱数据，参数为缓冲区的数据条数\n",
    "    map表示预处理,参数为与处理函数\n",
    "    \"\"\"\n",
    "    print(X_test.shape)\n",
    "    train_db = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "    train_db = train_db.shuffle(1000).map(preprocess).batch(128)\n",
    "    # 构建测试集对象，预处理，批量化\n",
    "    test_db = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "    test_db = test_db.map(preprocess).batch(64)\n",
    "    # 从训练集中采样一个 Batch，并观察\n",
    "    sample = next(iter(train_db))\n",
    "    #print('sample:', sample[0].shape, sample[1].shape, tf.reduce_min(sample[0]), tf.reduce_max(sample[0]))\n",
    "    return train_db, test_db\n",
    "\n",
    "def preprocess(x, y):\n",
    "    x = 2 * tf.cast(x, dtype=tf.float32) / 255. - 1\n",
    "    y = tf.cast(y, dtype=tf.int32)\n",
    "    return x, y\n",
    "\n",
    "def load_dataset1():\n",
    "    # 在线下载，加载 CIFAR10 数据集\n",
    "    (X_train, y_train), (X_test, y_test) = datasets.cifar10.load_data()\n",
    "    # 删除 y 的一个维度， [b,1] => [b]\n",
    "    #y_train = tf.squeeze(y_train, axis=1)\n",
    "    #y_test = tf.squeeze(y_test, axis=1)\n",
    "    #(X_train, y_train) = shuffle((X_train, y_train))\n",
    "    X_train, y_train = preprocess(X_train, y_train)\n",
    "    X_test, y_test = preprocess(X_test, y_test)\n",
    "    y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
    "    y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "#train_db, test_db = load_dataset()\n",
    "X_train, y_train, X_test, y_test = load_dataset1()\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def network_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(64, kernel_size=(3,3), padding=\"same\", activation=\"relu\", input_shape=[32, 32, 3]))\n",
    "    model.add(Conv2D(64, kernel_size=[3, 3], padding=\"same\", activation=tf.nn.relu))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=2, padding=\"same\"))\n",
    "        \n",
    "    model.add(Conv2D(128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "    model.add(Conv2D(128, kernel_size=[3, 3], padding=\"same\", activation=tf.nn.relu))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=2, padding=\"same\"))\n",
    "\n",
    "    model.add(Conv2D(256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "    model.add(Conv2D(256, kernel_size=[3, 3], padding=\"same\", activation=tf.nn.relu))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=2, padding=\"same\"))\n",
    "\n",
    "    model.add(Conv2D(512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "    model.add(Conv2D(512, kernel_size=[3, 3], padding=\"same\", activation=tf.nn.relu))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=2, padding=\"same\"))\n",
    "    \n",
    "    model.add(Conv2D(512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "    model.add(Conv2D(512, kernel_size=[3, 3], padding=\"same\", activation=tf.nn.relu))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=2, padding=\"same\"))\n",
    "    \n",
    "    model.add(Flatten())###把上层的输出拉平\n",
    "    model.add(Dense(256, activation=tf.nn.relu))\n",
    "    model.add(Dense(128, activation=tf.nn.relu))\n",
    "    model.add(Dense(10, activation=\"softmax\"))\n",
    "    \n",
    "    #model.summary()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = network_model()\n",
    "model.compile(loss=tf.keras.losses.categorical_crossentropy, optimizer=optimizers.Adam(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/15\n",
      "50000/50000 [==============================] - 23s 464us/sample - loss: 1.9039 - accuracy: 0.2498 - val_loss: 1.5362 - val_accuracy: 0.3994\n",
      "Epoch 2/15\n",
      "50000/50000 [==============================] - 18s 367us/sample - loss: 1.3257 - accuracy: 0.5049 - val_loss: 1.1625 - val_accuracy: 0.5738\n",
      "Epoch 3/15\n",
      "50000/50000 [==============================] - 18s 369us/sample - loss: 1.0013 - accuracy: 0.6417 - val_loss: 0.8927 - val_accuracy: 0.6908\n",
      "Epoch 4/15\n",
      "50000/50000 [==============================] - 18s 369us/sample - loss: 0.8034 - accuracy: 0.7195 - val_loss: 0.8000 - val_accuracy: 0.7273\n",
      "Epoch 5/15\n",
      "50000/50000 [==============================] - 19s 370us/sample - loss: 0.6732 - accuracy: 0.7681 - val_loss: 0.7794 - val_accuracy: 0.7379\n",
      "Epoch 6/15\n",
      "50000/50000 [==============================] - 18s 370us/sample - loss: 0.5634 - accuracy: 0.8059 - val_loss: 0.7065 - val_accuracy: 0.7689\n",
      "Epoch 7/15\n",
      "50000/50000 [==============================] - 19s 371us/sample - loss: 0.4822 - accuracy: 0.8343 - val_loss: 0.7551 - val_accuracy: 0.7595\n",
      "Epoch 8/15\n",
      "50000/50000 [==============================] - 19s 370us/sample - loss: 0.4105 - accuracy: 0.8584 - val_loss: 0.7634 - val_accuracy: 0.7674\n",
      "Epoch 9/15\n",
      "50000/50000 [==============================] - 19s 372us/sample - loss: 0.3489 - accuracy: 0.8798 - val_loss: 0.7439 - val_accuracy: 0.7785 loss: 0.3410 - accuracy:  - ETA: 6s - loss: 0.3410 - ac - ETA - ETA: 0s - loss: 0.3485 - accuracy\n",
      "Epoch 10/15\n",
      "50000/50000 [==============================] - 19s 372us/sample - loss: 0.2995 - accuracy: 0.8970 - val_loss: 0.8117 - val_accuracy: 0.7756\n",
      "Epoch 11/15\n",
      "50000/50000 [==============================] - 19s 372us/sample - loss: 0.2500 - accuracy: 0.9153 - val_loss: 0.9113 - val_accuracy: 0.7813: 0.2355 - accura - ETA: 6s - loss: 0.2425 - accura -\n",
      "Epoch 12/15\n",
      "50000/50000 [==============================] - 19s 372us/sample - loss: 0.2190 - accuracy: 0.9243 - val_loss: 0.8039 - val_accuracy: 0.7805- ETA: 12s - loss: 0.2007 - accur - ETA: 11s - los - ETA: 0s - loss: 0.2193 - accuracy: 0.92\n",
      "Epoch 13/15\n",
      "50000/50000 [==============================] - 19s 373us/sample - loss: 0.1957 - accuracy: 0.9327 - val_loss: 0.9041 - val_accuracy: 0.7771\n",
      "Epoch 14/15\n",
      "50000/50000 [==============================] - 19s 372us/sample - loss: 0.1775 - accuracy: 0.9416 - val_loss: 0.9109 - val_accuracy: 0.7868\n",
      "Epoch 15/15\n",
      "50000/50000 [==============================] - 19s 372us/sample - loss: 0.1561 - accuracy: 0.9490 - val_loss: 0.9518 - val_accuracy: 0.7828\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x243b83bba48>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, batch_size=128, epochs=15, validation_data=(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
